---
title: "üåê HTTP Transport"
description: "Complete guide to using Bifrost as an HTTP API service with **built-in web UI**, zero-configuration startup, multi-provider AI access, drop-in integrations, and production deployment."
---

# üåê HTTP Transport

Complete guide to using Bifrost as an HTTP API service with **built-in web UI**, zero-configuration startup, multi-provider AI access, drop-in integrations, and production deployment.

> **üí° Quick Start:** See the [15-second zero-config setup](../../quickstart/http-transport) to get the HTTP service running with web UI instantly.

---

## üìã HTTP Transport Overview

Bifrost HTTP transport provides a REST API service with **built-in web UI** for:

- **üñ•Ô∏è Visual configuration** with real-time monitoring and analytics
- **üöÄ Zero-configuration startup** - begin immediately, configure dynamically
- **üîÑ Multi-provider access** through unified endpoints
- **üîó Drop-in replacements** for OpenAI, Anthropic, Google GenAI APIs
- **üåê Language-agnostic integration** with any HTTP client
- **üìä Production-ready deployment** with monitoring and scaling
- **üõ†Ô∏è MCP tool execution** via HTTP endpoints

```bash
# Start Bifrost HTTP service (zero config!)
docker run -p 8080:8080 maximhq/bifrost

# Open web interface for visual configuration
open http://localhost:8080

# Make requests to any provider
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "openai/gpt-4o-mini", "messages": [...]}'
```

---

## üöÄ Core Features

### **Unified API Endpoints**

| Endpoint                    | Purpose            | Documentation                     |
| --------------------------- | ------------------ | --------------------------------- |
| `POST /v1/chat/completions` | Chat conversations | [Endpoints Guide](./endpoints) |
| `POST /v1/text/completions` | Text generation    | [Endpoints Guide](./endpoints) |
| `POST /v1/mcp/tool/execute` | Tool execution     | [Endpoints Guide](./endpoints) |
| `GET /metrics`              | Prometheus metrics | [Endpoints Guide](./endpoints) |

### **Drop-in API Compatibility**

| Provider         | Endpoint                            | Compatibility                                                  |
| ---------------- | ----------------------------------- | -------------------------------------------------------------- |
| **OpenAI**       | `POST /openai/v1/chat/completions`  | [OpenAI Compatible](./integrations/openai-compatible)       |
| **Anthropic**    | `POST /anthropic/v1/messages`       | [Anthropic Compatible](./integrations/anthropic-compatible) |
| **Google GenAI** | `POST /genai/v1beta/models/{model}` | [GenAI Compatible](./integrations/genai-compatible)         |

> **üìñ Migration:** See [Migration Guide](./integrations/migration-guide) for step-by-step migration from existing providers.

---

## ‚öôÔ∏è Configuration

### **Core Configuration Files**

| Component                                        | Configuration                   | Time to Setup |
| ------------------------------------------------ | ------------------------------- | ------------- |
| **[üîß Providers](./configuration/providers)** | API keys, models, fallbacks     | 5 min         |
| **[üõ†Ô∏è MCP Integration](./configuration/mcp)** | Tool servers and connections    | 10 min        |
| **[üîå Plugins](./configuration/plugins)**     | Custom middleware (coming soon) | 5 min         |

### **Quick Configuration Example**

```json
{
  "providers": {
    "openai": {
      "keys": [
        {
          "value": "env.OPENAI_API_KEY",
          "models": ["gpt-4o-mini"],
          "weight": 1.0
        }
      ]
    },
    "anthropic": {
      "keys": [
        {
          "value": "env.ANTHROPIC_API_KEY",
          "models": ["claude-3-sonnet-20240229"],
          "weight": 1.0
        }
      ]
    }
  },
  "mcp": {
    "client_configs": [
      {
        "name": "filesystem",
        "connection_type": "stdio",
        "stdio_config": {
          "command": "npx",
          "args": ["-y", "@modelcontextprotocol/server-filesystem"]
        }
      }
    ]
  }
}
```

---

### Runtime logging

Control verbosity and output format via CLI flags when starting the server:

- `-log-level`: debug | info | warn | error (default: info)
- `-log-style`: json | pretty (default: json)

Examples:

```bash
# Human-friendly console logs at debug level
npx -y @maximhq/bifrost -log-level debug -log-style pretty

# Docker with pretty logs for local debugging
docker run -p 8080:8080 maximhq/bifrost -log-level debug -log-style pretty

# Production-friendly structured logs
docker run -p 8080:8080 maximhq/bifrost -log-level info -log-style json
```

Notes:

- `pretty` is easier to read locally; `json` is best for log aggregation.
- Log level controls which messages are emitted at runtime; lower levels include higher ones (e.g., debug includes info/warn/error).

---

## üîó Integration Patterns

### **"I want to..."**

| Goal                       | Integration Type       | Guide                                                          |
| -------------------------- | ---------------------- | -------------------------------------------------------------- |
| **Replace OpenAI API**     | Drop-in replacement    | [OpenAI Compatible](./integrations/openai-compatible)       |
| **Replace Anthropic API**  | Drop-in replacement    | [Anthropic Compatible](./integrations/anthropic-compatible) |
| **Use with existing SDKs** | Change base URL only   | [Migration Guide](./integrations/migration-guide)           |
| **Add multiple providers** | Provider configuration | [Providers Config](./configuration/providers)               |
| **Add external tools**     | MCP integration        | [MCP Config](./configuration/mcp)                           |
| **Custom monitoring**      | Plugin configuration   | [Plugins Config](./configuration/plugins)                   |
| **Production deployment**  | Docker + config        | [Deployment Guide](../../quickstart/http-transport)         |

### **Language Examples**

<details>
<summary><strong>Python (OpenAI SDK)</strong></summary>

```python
from openai import OpenAI

# Option 1: Change base URL to use Bifrost with configured keys
client = OpenAI(
    base_url="http://localhost:8080/openai",  # Point to Bifrost
    api_key="your-openai-key"  # Will be passed in Authorization header
)

# Option 2: Use Bifrost unified API with header-based keys

> Note: This requires enabling "Allow Direct Keys" in the Bifrost UI (Settings) or setting `allow_direct_keys: true` in client config.

import requests

response = requests.post(
    "http://localhost:8080/v1/chat/completions",
    headers={
        "Content-Type": "application/json",
        "Authorization": "Bearer sk-your-openai-key"  # Bearer format required
    },
    json={
        "model": "openai/gpt-4o-mini",
        "messages": [{"role": "user", "content": "Hello!"}]
    }
)

# Use normally - Bifrost handles provider routing
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

</details>

<details>
<summary><strong>JavaScript/Node.js</strong></summary>

```javascript
import OpenAI from "openai";

// Option 1: Use OpenAI SDK with Bifrost
const openai = new OpenAI({
  baseURL: "http://localhost:8080/openai", // Point to Bifrost
  apiKey: process.env.OPENAI_API_KEY,      // Passed in Authorization header
});

const response = await openai.chat.completions.create({
  model: "gpt-4o-mini",
  messages: [{ role: "user", content: "Hello!" }],
});

// Option 2: Use fetch with direct headers
const response = await fetch("http://localhost:8080/v1/chat/completions", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "Authorization": `Bearer ${process.env.OPENAI_API_KEY}` // Bearer format required
  },
  body: JSON.stringify({
    model: "openai/gpt-4o-mini",
    messages: [{ role: "user", content: "Hello!" }]
  })
});
```

</details>

<details>
<summary><strong>cURL</strong></summary>

```bash
# Direct Bifrost API
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai/gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello!"}],
    "fallbacks": ["anthropic/claude-3-sonnet-20240229"]
  }'

# OpenAI-compatible endpoint
curl -X POST http://localhost:8080/openai/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

</details>

---

## üöÄ Deployment Options

### **Zero-Config Docker (Recommended)**

```bash
# Start instantly with web UI
docker run -p 8080:8080 maximhq/bifrost
# Configure via http://localhost:8080
```

### **File-Based Docker**

```bash
# With persistent config.json in app directory
docker run -p 8080:8080 \
  -v $(pwd):/app/data \
  -e OPENAI_API_KEY \
  -e ANTHROPIC_API_KEY \
  maximhq/bifrost
```

### **Binary Deployment**

```bash
# Zero config startup (uses current directory)
npx -y @maximhq/bifrost -port 8080
```

For detailed deployment instructions including app directory setup, Docker volumes, and production best practices, see:

- [Understanding App Directory & Docker Volumes](../../quickstart/http-transport#understanding-app-directory--docker-volumes)
- [Production Deployment Guide](../../quickstart/http-transport#production-deployment)

---

## üìä Monitoring and Observability

### **Built-in Metrics**

```bash
# Prometheus metrics endpoint
curl http://localhost:8080/metrics

# Key metrics available:
# - bifrost_requests_total{provider, model, status}
# - bifrost_request_duration_seconds{provider, model}
# - bifrost_tokens_total{provider, model, type}
# - bifrost_errors_total{provider, error_type}
```

### **Health Checks**

```bash
# Basic health check
curl http://localhost:8080/v1/chat/completions \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{"provider":"openai","model":"gpt-4o-mini","messages":[{"role":"user","content":"test"}]}'
```

---

## üìö Complete Documentation

### **üìñ API Reference**

- **[üåê Endpoints](./endpoints)** - Complete API endpoint documentation
- **[üìã OpenAPI Spec](./openapi.json)** - Machine-readable API specification

### **‚öôÔ∏è Configuration Guides**

- **[üîß Provider Setup](./configuration/providers)** - Configure AI providers and keys
- **[üõ†Ô∏è MCP Integration](./configuration/mcp)** - Setup external tool integration
- **[üîå Plugin System](./configuration/plugins)** - Configure custom middleware

### **üîó Integration Guides**

- **[üì± Drop-in Integrations](./integrations/README)** - Overview of API compatibility
- **[üîÑ Migration Guide](./integrations/migration-guide)** - Migrate from existing providers
- **[‚öôÔ∏è SDK Examples](./integrations/)** - Language-specific integration examples

---

## üéØ Next Steps

1. **[‚ö° Quick Setup](../../quickstart/http-transport)** - Get Bifrost HTTP running in 30 seconds
2. **[üîß Configure Providers](./configuration/providers)** - Add your AI provider credentials
3. **[üîó Choose Integration](./integrations/README)** - Pick drop-in replacement or unified API
4. **[üöÄ Deploy to Production](../../quickstart/http-transport#production-deployment)** - Scale for production workloads

> **üèõÔ∏è Architecture:** For HTTP transport design and performance details, see [Architecture Documentation](../../architecture/README).

---

## üìö Additional Resources

- [Configuration Guide](./configuration/providers)
- [API Endpoints](./endpoints)
- [Error Handling](../errors)
- [Monitoring & Metrics](./configuration/plugins)
