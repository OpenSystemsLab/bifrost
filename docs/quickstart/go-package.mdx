---
title: "Go Package Quick Start"
description: "Get Bifrost running in your Go application in 30 seconds with minimal setup and direct code integration."
---

# 🔧 Go Package Quick Start

Get Bifrost running in your Go application in 30 seconds with this minimal setup guide.

![Bifrost Go Package Demo Video](../media/package-demo.mp4)

## ⚡ 30-Second Setup

### 1. Install Package

```bash
go mod init my-bifrost-app
go get github.com/maximhq/bifrost/core
```

### 2. Set Environment Variable

```bash
export OPENAI_API_KEY="your-openai-api-key"
```

### 3. Create `main.go`

```go
package main

import (
    "context"
    "fmt"
    "os"

    "github.com/maximhq/bifrost/core"
    "github.com/maximhq/bifrost/core/schemas"
)

type MyAccount struct{}

func (a *MyAccount) GetKey(provider, model string) (string, error) {
    return os.Getenv("OPENAI_API_KEY"), nil
}

func main() {
    // Initialize Bifrost
    client, err := bifrost.Init(schemas.BifrostConfig{
        Account: &MyAccount{},
    })
    if err != nil {
        panic(err)
    }

    // Make a request
    response, err := client.ChatCompletion(context.Background(), schemas.ChatCompletionRequest{
        Provider: "openai",
        Model:    "gpt-4o-mini",
        Messages: []schemas.Message{
            {Role: "user", Content: "Hello, Bifrost!"},
        },
    })
    if err != nil {
        panic(err)
    }

    fmt.Println("Response:", response.Choices[0].Message.Content)
}
```

### 4. Run Your App

```bash
go run main.go
# Output: Response: Hello! I'm Bifrost, your AI model gateway...
```

**🎉 That's it!** You're now running Bifrost in your Go application.

---

## 🎯 What Just Happened?

1. **Account Interface**: `MyAccount` provides API keys to Bifrost
2. **Provider Resolution**: `"openai"` automatically routes to OpenAI
3. **Model Selection**: `"gpt-4o-mini"` specifies which model to use
4. **Unified API**: Same interface works for any provider (OpenAI, Anthropic, etc.)

---

## 🚀 Add More Providers (2 minutes)

Add multiple providers for automatic failover:

```go
type MyAccount struct{}

func (a *MyAccount) GetKey(provider, model string) (string, error) {
    keys := map[string]string{
        "openai":    os.Getenv("OPENAI_API_KEY"),
        "anthropic": os.Getenv("ANTHROPIC_API_KEY"),
        "bedrock":   os.Getenv("AWS_ACCESS_KEY_ID"),
    }
    
    if key, exists := keys[provider]; exists && key != "" {
        return key, nil
    }
    
    return "", fmt.Errorf("no API key for provider: %s", provider)
}
```

Now Bifrost automatically tries different providers if one fails:

```go
// This will automatically failover between providers
response, err := client.ChatCompletion(context.Background(), schemas.ChatCompletionRequest{
    Provider: "openai", // Will try Anthropic/Bedrock if OpenAI fails
    Model:    "gpt-4o-mini",
    Messages: []schemas.Message{
        {Role: "user", Content: "Hello!"},
    },
})
```

---

## 🛠️ Add External Tools with MCP (2 minutes)

Enable your AI to use external tools:

```go
import "github.com/maximhq/bifrost/core/mcp"

func main() {
    client, err := bifrost.Init(schemas.BifrostConfig{
        Account: &MyAccount{},
        MCP: &schemas.MCPConfig{
            Servers: []schemas.MCPServer{
                {
                    Name: "filesystem",
                    Command: []string{"node", "/path/to/filesystem-server.js"},
                },
            },
        },
    })
    if err != nil {
        panic(err)
    }

    // AI can now use file system tools
    response, err := client.ChatCompletion(context.Background(), schemas.ChatCompletionRequest{
        Provider: "openai",
        Model:    "gpt-4o-mini",
        Messages: []schemas.Message{
            {Role: "user", Content: "List the files in the current directory"},
        },
        ToolChoice: "auto", // Enable automatic tool usage
    })
}
```

---

## 🔌 Add Custom Plugins (3 minutes)

Add custom middleware for logging, caching, etc.:

```go
type LoggingPlugin struct{}

func (p *LoggingPlugin) PreHook(ctx context.Context, req *schemas.BifrostRequest) (*schemas.BifrostRequest, *schemas.PluginShortCircuit, error) {
    fmt.Printf("[LOG] Request to %s/%s\n", req.Provider, req.Model)
    return req, nil, nil
}

func (p *LoggingPlugin) PostHook(ctx context.Context, response *schemas.BifrostResponse, err *schemas.BifrostError) (*schemas.BifrostResponse, *schemas.BifrostError, error) {
    if err != nil {
        fmt.Printf("[LOG] Error: %s\n", err.Error.Message)
    } else {
        fmt.Printf("[LOG] Success: %d tokens\n", response.Usage.TotalTokens)
    }
    return response, err, nil
}

func main() {
    client, err := bifrost.Init(schemas.BifrostConfig{
        Account: &MyAccount{},
        Plugins: []schemas.Plugin{
            &LoggingPlugin{},
        },
    })
    // Now all requests will be logged
}
```

---

## 💼 Production Example

Here's a more complete production setup:

```go
package main

import (
    "context"
    "log"
    "os"
    "time"

    "github.com/maximhq/bifrost/core"
    "github.com/maximhq/bifrost/core/schemas"
)

type ProductionAccount struct {
    keys map[string]string
}

func NewProductionAccount() *ProductionAccount {
    return &ProductionAccount{
        keys: map[string]string{
            "openai":    os.Getenv("OPENAI_API_KEY"),
            "anthropic": os.Getenv("ANTHROPIC_API_KEY"),
            "bedrock":   os.Getenv("AWS_ACCESS_KEY_ID"),
        },
    }
}

func (a *ProductionAccount) GetKey(provider, model string) (string, error) {
    if key, exists := a.keys[provider]; exists && key != "" {
        return key, nil
    }
    return "", fmt.Errorf("no API key configured for provider: %s", provider)
}

type MetricsPlugin struct{}

func (p *MetricsPlugin) PostHook(ctx context.Context, response *schemas.BifrostResponse, err *schemas.BifrostError) (*schemas.BifrostResponse, *schemas.BifrostError, error) {
    // Track metrics (tokens, latency, errors)
    if response != nil {
        log.Printf("Tokens used: %d", response.Usage.TotalTokens)
    }
    return response, err, nil
}

func main() {
    client, err := bifrost.Init(schemas.BifrostConfig{
        Account: NewProductionAccount(),
        Plugins: []schemas.Plugin{
            &MetricsPlugin{},
        },
        MCP: &schemas.MCPConfig{
            Servers: []schemas.MCPServer{
                {
                    Name: "filesystem",
                    Command: []string{"npx", "@modelcontextprotocol/server-filesystem", "/tmp"},
                },
            },
        },
        Memory: &schemas.MemoryConfig{
            BufferSize:      10000,
            InitialPoolSize: 5000,
        },
    })
    if err != nil {
        log.Fatal("Failed to initialize Bifrost:", err)
    }

    // Your application logic here
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()

    prompt := "What's the current time?"
    response, berr := client.ChatCompletionRequest(ctx, &schemas.BifrostRequest{
        Provider: schemas.OpenAI,
        Model:    "gpt-4o-mini",
        Input: schemas.RequestInput{
            ChatCompletionInput: &[]schemas.BifrostMessage{
                {
                    Role:    schemas.ModelChatMessageRoleUser,
                    Content: schemas.MessageContent{ContentStr: &prompt},
                },
            },
        },
        // Example: enable tools if configured
        // Tools: &[]schemas.Tool{ ... },
    })
    if berr != nil {
        log.Fatal("Chat completion failed:", berr)
    }
    if len(response.Choices) > 0 && response.Choices[0].Message.Content.ContentStr != nil {
        log.Printf("Response: %s", *response.Choices[0].Message.Content.ContentStr)
    }    
    if err != nil {
        log.Fatal("Chat completion failed:", err)
    }

    log.Printf("Response: %s", response.Choices[0].Message.Content)
}
```

---

## 📚 Next Steps

Now that you have Bifrost running, explore these guides:

### 🏗️ Core Concepts
- **[Account Management](../usage/go-package/account)** - Advanced API key management
- **[Provider Configuration](../usage/providers)** - Configure multiple AI providers
- **[Error Handling](../usage/errors)** - Robust error handling patterns

### 🛠️ Advanced Features  
- **[MCP Integration](../usage/go-package/mcp)** - Complete MCP setup and examples
- **[Plugin Development](../usage/go-package/plugins)** - Build custom plugins
- **[Memory Management](../usage/memory-management)** - Optimize performance

### 🎯 Use Cases
- **[Streaming Responses](../usage/go-package/bifrost-client)** - Handle streaming completions
- **[Tool Calling](../mcp)** - Enable AI to use external tools
- **[Multi-turn Conversations](../usage/go-package/schemas)** - Conversation management

---

## 🔧 Troubleshooting

### Common Issues

**Module not found**
```bash
go mod tidy
go get github.com/maximhq/bifrost/core@latest
```

**API key errors**
```bash
# Check environment variables
echo $OPENAI_API_KEY

# Test key validity
curl -H "Authorization: Bearer $OPENAI_API_KEY" https://api.openai.com/v1/models
```

**Build errors**
```bash
# Update to latest Go version
go version  # Should be 1.21+

# Clean module cache
go clean -modcache
go mod download
```

### Getting Help

- **[Complete Go Package Guide](../usage/go-package/)** - Detailed documentation
- **[Architecture Overview](../architecture/)** - How Bifrost works internally
- **[Examples Repository](https://github.com/maximhq/bifrost-examples)** - Working code examples

**Still stuck?** [Open an issue](https://github.com/maximhq/bifrost/issues) with your configuration and error details.

---

## 🎉 What's Next?

You're now ready to:

1. **[Add More Providers](../usage/providers)** for redundancy and cost optimization
2. **[Enable MCP Tools](../mcp)** to give AI models external capabilities
3. **[Build Custom Plugins](../plugins)** for your specific business logic
4. **[Explore Advanced Features](../usage/go-package/)** for production deployment

**Happy coding with Bifrost!** 🚀
