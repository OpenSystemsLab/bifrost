---
title: "HTTP Transport Quick Start"
description: "Get Bifrost running as an HTTP API in 15 seconds with zero configuration, perfect for any programming language."
---

# üåê HTTP Transport Quick Start

Get Bifrost running as an HTTP API in **15 seconds** with **zero configuration**! Perfect for any programming language.

## üöÄ Zero-Config Setup (15 seconds!)

### 1. Start Bifrost (No config needed)

```bash
# üê≥ Docker
docker pull maximhq/bifrost
docker run -p 8080:8080 maximhq/bifrost

# üîß OR Binary
npx @maximhq/bifrost  # use -port flag to specify the port
```

### Optional: Logging for local debugging

Control verbosity and output format while developing:

```bash
# Human-friendly console formatting
npx @maximhq/bifrost -log-level debug -log-style pretty

# Structured logs (recommended for prod)
npx @maximhq/bifrost -log-level info -log-style json
```

### 2. Open the Web Interface

Navigate to **http://localhost:8080** in your browser:

- **Configure providers** with a visual interface  
- **Test requests** directly in the browser
- **Monitor usage** in real-time
- **View logs** and debug information

### 3. Test with a Simple Request

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Hello, Bifrost!"}
    ]
  }'
```

**üéâ That's it!** Bifrost is running and ready to route AI requests.

---

## üîë Add Your API Keys (1 minute)

### Option 1: Via Web Interface
1. Go to **http://localhost:8080**
2. Click **"Providers"** in the sidebar
3. Add your API keys for OpenAI, Anthropic, etc.
4. Click **"Save Configuration"**

### Option 2: Via Environment Variables

```bash
# Set your API keys
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export AWS_ACCESS_KEY_ID="AKIA..."
export AWS_SECRET_ACCESS_KEY="..."

# Restart Bifrost to pick up the keys
npx @maximhq/bifrost
```

### Option 3: Via Configuration File

Create `config.json`:

```json
{
  "providers": {
    "openai": {
      "api_key": "sk-..."
    },
    "anthropic": {
      "api_key": "sk-ant-..."
    },
    "bedrock": {
      "aws_access_key_id": "AKIA...",
      "aws_secret_access_key": "...",
      "aws_region": "us-east-1"
    }
  }
}
```

Start with config:

```bash
npx @maximhq/bifrost -config config.json
```

---

## üîÑ Drop-in API Compatibility

Bifrost provides **100% compatible** endpoints for major AI providers:

### OpenAI Compatible

```python
import openai

# Just change the base_url - everything else stays the same!
client = openai.OpenAI(
    base_url="http://localhost:8080/openai",
    api_key="dummy-key"  # Not needed with Bifrost
)

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### Anthropic Compatible

```python
import anthropic

client = anthropic.Anthropic(
    base_url="http://localhost:8080/anthropic",
    api_key="dummy-key"
)

response = client.messages.create(
    model="claude-3-sonnet-20240229",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### GenAI Compatible

```python
import google.generativeai as genai

genai.configure(
    api_key="dummy-key",
    transport="rest",
    client_options={"api_endpoint": "http://localhost:8080/genai"}
)

model = genai.GenerativeModel("gemini-pro")
response = model.generate_content("Hello!")
```

---

## üõ†Ô∏è Enable External Tools with MCP (2 minutes)

Let AI models use external tools like file systems, web search, etc.

### 1. Configure MCP via Web Interface

1. Go to **http://localhost:8080**
2. Click **"MCP Clients"** in the sidebar  
3. Add MCP servers (filesystem, web search, etc.)
4. Save configuration

### 2. Or configure via JSON:

```json
{
  "mcp": {
    "servers": [
      {
        "name": "filesystem",
        "command": ["npx", "@modelcontextprotocol/server-filesystem", "/tmp"]
      },
      {
        "name": "web-search", 
        "command": ["npx", "@modelcontextprotocol/server-web-search"]
      }
    ]
  }
}
```

### 3. Test tool usage:

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "List the files in /tmp directory"}
    ]
  }'
```

The AI will automatically use the filesystem tool to list files!

---

## üéâ What's Next?

You're now ready to:

1. **[Configure Multiple Providers](../usage/providers)** for redundancy and cost optimization
2. **[Enable MCP Tools](../mcp)** to give AI models external capabilities  
3. **[Set Up Governance](../governance)** for user management and cost controls
4. **[Deploy to Production](../usage/http-transport/configuration/)** with proper monitoring

**Happy building with Bifrost!** üöÄ
